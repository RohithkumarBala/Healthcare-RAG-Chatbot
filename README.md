ğŸ©º Healthcare RAG Chatbot (Free-Tier, Production-Safe)

An end-to-end Retrieval-Augmented Generation (RAG) chatbot that allows users to ask questions from healthcare PDF documents.
The system is designed to work entirely on free tiers, with robust fallbacks to avoid runtime failures.

ğŸš€ Project Overview

This project demonstrates a real-world RAG pipeline used in healthcare document analysis:

Upload healthcare PDFs (guidelines, reports, notes)

Convert documents into embeddings

Store embeddings in a vector database

Retrieve relevant context for each query

Generate accurate answers using LLMs

The architecture avoids unstable LangChain abstractions and uses LCEL (Runnable pipelines) for long-term stability.

ğŸ§  Architecture (High Level)
PDFs
  â†“
Text Splitter
  â†“
Embeddings (Gemini â†’ HuggingFace fallback)
  â†“
ChromaDB (Vector Store)
  â†“
Retriever (Top-K similarity search)
  â†“
LLM (Gemini â†’ HuggingFace / Ollama fallback)
  â†“
Streamlit UI

ğŸ› ï¸ Tech Stack

Python

LangChain (LCEL / Runnable architecture)

Google Gemini API (Primary LLM + Embeddings)

HuggingFace Embeddings (Fallback)

Ollama (Local LLM â€“ Optional)

ChromaDB (Vector Database)

Streamlit (UI)

PyPDF (PDF ingestion)

ğŸ“‚ Project Structure
Healthcare_RAG_Chatbot/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ ingest.py              # PDF ingestion + embedding
â”œâ”€â”€ rag_chain.py           # RAG pipeline (LCEL-based)
â”œâ”€â”€ embeddings_loader.py   # Embedding fallback logic
â”œâ”€â”€ llm_loader.py          # LLM fallback logic
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                   # API keys (not committed)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_docs/       # Healthcare PDFs
â”‚
â””â”€â”€ chroma_db/             # Vector database (auto-generated)

âš™ï¸ Setup Instructions
1ï¸âƒ£ Create Virtual Environment (Recommended)
python -m venv venv
venv\Scripts\activate   # Windows

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Configure Environment Variables

Create a .env file:

GOOGLE_API_KEY=your_gemini_api_key
HUGGINGFACEHUB_API_TOKEN=your_hf_token

4ï¸âƒ£ Add Healthcare PDFs

Place PDFs inside:

data/sample_docs/

5ï¸âƒ£ Ingest Documents
python ingest.py


This will:

Load PDFs

Split text into chunks

Generate embeddings

Store them in ChromaDB

6ï¸âƒ£ Run the App
streamlit run app.py

ğŸ’¬ Example Questions to Ask

â€œWhat is the main topic discussed in the document?â€

â€œSummarize the key medical recommendations.â€

â€œWhat are the risks or precautions mentioned?â€

â€œExplain this document in simple terms.â€

ğŸ–¥ï¸ UI Demo

![alt text](image-3.png)
![alt text](image-4.png)



ğŸ” Local vs Cloud Model Strategy (IMPORTANT)

This project is intentionally designed to work in both local and cloud environments.

â˜ï¸ Cloud Mode (Recommended for Deployment)

Used when:

Deploying to Streamlit Cloud / AWS / GCP

Demonstrating to recruiters

Models:

âœ… Gemini (Primary)

âœ… HuggingFace (Fallback)

Why:

No hardware dependency

Stable on free tiers

Works in production environments

ğŸ’» Local Mode (Optional â€“ Development Only)

Used when:

Experimenting locally

Learning about local LLMs

Model:

Ollama (e.g., phi3, quantized models)

Important Notes:

Requires Ollama to be running on localhost:11434

Needs sufficient RAM

Not suitable for cloud deployment

Disabled by default in production logic

Ollama is treated as an optional local enhancement, not a hard dependency.

ğŸ§© Key Engineering Decisions

âŒ Avoided langchain.chains (frequent breaking changes)

âœ… Used LCEL (Runnable pipelines) for stability

âœ… Normalized LLM outputs (str vs AIMessage)

âœ… Graceful fallback logic to prevent crashes

âœ… Free-tier friendly design

ğŸ“ˆ Why This Project Matters

This project demonstrates:

Practical RAG implementation

Handling API rate limits & failures

Production-safe fallback strategies

Awareness of local vs cloud constraints

Clean architecture suitable for real deployments

ğŸ”® Future Enhancements

Source document citations in UI

Chat history memory

User PDF upload from UI

Role-based access (doctor / patient views)

Cloud deployment (Streamlit Cloud / AWS)

ğŸ‘¤ Author

Rohithkumar Bala
Aspiring AI Engineer | GenAI | RAG | LLM Systems
